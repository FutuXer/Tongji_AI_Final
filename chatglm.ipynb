from transformers import TextStreamer, AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "/mnt/workspace/chatglm3-6b"  # 本地模型路径

prompt = "领导：你这是什么意思？ 小明：没什么意思。意思意思。 领导：你这就不够意思了。 小明：小意思，小意思。领导：你这人真有意思。 小明：其实也没有别的意思。 领导：那我就不好意思了。 小明：是我不好意思。请问：以上“意思”分别是什么意思。"

# 加载 tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    local_files_only=True  
)

# 加载模型，自动选dtype
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None,
    local_files_only=True
).eval()

# 编码输入
inputs = tokenizer(prompt, return_tensors="pt").input_ids
if torch.cuda.is_available():
    inputs = inputs.cuda()

# 准备文本流式输出器
streamer = TextStreamer(tokenizer)

# 生成
outputs = model.generate(
    inputs,
    max_new_tokens=300,
    streamer=streamer,
    do_sample=False,
    num_beams=1,
    eos_token_id=tokenizer.eos_token_id,
)

# 输出完整答案
generated_tokens = outputs[0][inputs.shape[1]:]
response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
print("\n模型完整回复：", response)